# -*- coding: utf-8 -*-
"""tradify2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-c9A_eTcpkCuYyTfxf1UV_QJhukkPUwu

**IMPORTANT LIBRARY**
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install numpy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM , Dense , Dropout
import os
import tensorflow as tf
import pickle

tf.__version__

"""**TABLE CONTENT**"""

filename = "REL(2).csv"
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/"+filename)
df.head()

ex = df['High'].values

type(ex)

ex = ex.reshape(-1,1)
# WE ARE USING -1 BECAUSE WE  DONT KNOW THE SIZE OF DATA WE CAN GET

ex.shape

"""**CREATION OF TRAINING AND TEST DATA SET**"""

dataset_train =  np.array(ex[:int(ex.shape[0]*0.8)])
dataset_test =  np.array(ex[int(ex.shape[0]*0.8)-50:])
# we took first 80% data for training and rest 20% for testing
# and made 50 data overlap

print(dataset_train.shape)
dataset_test.shape

"""**PREPROCESSING THE DATSET**"""

scaler = MinMaxScaler(feature_range=(0,1))
#this mean our data can only have values min=0 and max =1
dataset_test_out = dataset_test
dataset_train = scaler.fit_transform(dataset_train)
dataset_test = scaler.transform(dataset_test)

"""***CREATE MY DATASET FUNCTION***"""

#THIS FUNCTION OBJECTIVE IS TO TAKE 50 DATATSET AND TRY TO PREDICT 51ST DAY

def create_my_dataset(ex):
  x = []
  y = []
  for i in range(50,ex.shape[0]):
    x.append(ex[i-50:i,0])
    y.append(ex[i,0])
  x = np.array(x)
  y = np.array(y)
  return x,y

x_train,y_train = create_my_dataset(dataset_train)
x_test,y_test = create_my_dataset(dataset_test)

print(x_train.shape)
print(x_train[:1])
#we are taking 50 days record and will try to predict 51st day
# than we will move to 1st day and will try to predict 52nd day using
# last 50 days record

x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))
x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))
print(x_train.shape)
x_test.shape

"""**TRAINING/FEEDING THE NETWORK**

---


"""

model = Sequential()
model.add(LSTM(units=90, return_sequences=True,input_shape=(x_train.shape[1],1)))
model.add(Dropout(0.2))
model.add(LSTM(units=90, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=90))
model.add(Dropout(0.2))
model.add(Dense(units=1))
# units = amount of neuron
# input shape is 50 because we will give 50 day data
# to predict 51st
# 20% will be dead neurons

"""**MODEL STRUCTURE**"""

model.summary()



model.compile(loss = 'mean_squared_error',optimizer='adam')

"""FEEDING NETWORK"""

if (not os.path.exists("/content/drive/MyDrive/Colab Notebooks/LSTM/stock_prediction.h5")):
  model.fit(x_train,y_train, epochs=50, batch_size=32)
  model.save("/content/drive/MyDrive/Colab Notebooks/LSTM/stock_prediction.h5")

"""**VISUALISING THE NETWORK RESULT**"""

model = load_model('/content/drive/MyDrive/Colab Notebooks/LSTM/stock_prediction.h5')

predictions = model.predict(x_test)
#inverse the scaling of 0.778 to 70$
predictions = scaler.inverse_transform(predictions)
predictions
#dataset_test_out.shape

"""# New Section"""

fig,ax = plt.subplots(figsize=(8,4))
plt.plot(ex,color='red',label='original Stockprice')

ax.plot(range(len(y_train)+50,len(y_train)+50+len(predictions)),predictions, color='blue', label='predictions')
plt.legend()
print(range(len(y_train)+50,len(y_train)+50+len(predictions)))

y_test_scaled = scaler.inverse_transform(y_test.reshape(-1,1))
fig,ax = plt.subplots(figsize=(8,4))
ax.plot(y_test_scaled, color='red', label='TRUE price')
plt.plot(predictions,color="blue",label = "prediction prices")
plt.legend()

predictions[-1]



